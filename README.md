# Create LLM Examples with JAX and Flax

## Project Description

JAX is a Python library for high-performance computing. It’s popular for machine learning research and used at Google to develop Gemini models. There are libraries built on top of JAX to help developers create neural networks, including Flax. You can find a good example using `flax.nnx` in this tutorial, which shows you how to train “miniGPT” (a “hello world” large language model) from scratch on a TPU.

In this project, you’ll create additional notebooks that teach developers how to train and optimize LLMs using JAX and Flax. We are primarily interested in educational content (e.g., notebooks that teach developers new skills and help them on their journey). To be clear, you are not expected to develop advanced LLMs from scratch -- rather, you will work with a mentor to create additional educational examples like the above, and to expand and optimize the examples we already have.

This project is right for you if you’re interested in learning more about JAX, and training and optimizing large language models, and enjoy teaching developers. It’s a good way to build your portfolio of notebooks on GitHub, too.

## Project Tasks

- Create clear notebooks in JAX and Flax and publish your examples on GitHub.
- Create examples that show how to implement components of large language models, and clearly explain how they work.
- Create examples that show how to optimize performance when using GPUs and TPUs for training and inference.

## Difficulty

**Hard**

## Time Commitment

**Flexible**

## Eligibility

Prior experience working with a machine learning framework (JAX, Keras, TensorFlow, or PyTorch) and Jupyter Notebooks. You don’t need to have trained advanced models or anything like that, but you should know the basics of at least one library.

## How to Apply

Please include a paragraph on your experience with machine learning, and large language models (if you happen to have some, it’s not required).
