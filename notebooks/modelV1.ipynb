{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QrWmX-EyN3um"
      },
      "outputs": [],
      "source": [
        "# !pip install -q jax-ai-stack\n",
        "# !pip install -Uq tiktoken grain matplotlib\n",
        "# !pip install grain\n",
        "\n",
        "\n",
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5M2Cxi_6N3uo"
      },
      "outputs": [],
      "source": [
        "# import jax\n",
        "# jax.devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-jGF3qn0N3up"
      },
      "outputs": [],
      "source": [
        "# !wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true -O TinyStories-train.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sBPCSlCeN3uq"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax\n",
        "from dataclasses import dataclass\n",
        "import grain.python as pygrain\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kwLYPHmJN3uq"
      },
      "outputs": [],
      "source": [
        "mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TvE_CdExN3ur"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
        "                                          in_features=embed_dim,\n",
        "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                          rngs=rngs)\n",
        "        self.dropout1 = nnx.Dropout(rate=rate)\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
        "                                  out_features=ff_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
        "                                  out_features=embed_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  rngs=rngs)\n",
        "        self.dropout2 = nnx.Dropout(rate=rate)\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         rngs=rngs)\n",
        "\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        _, seq_len, _ = input_shape\n",
        "\n",
        "        # Create causal mask\n",
        "        mask = causal_attention_mask(seq_len)\n",
        "\n",
        "        # Apply MultiHeadAttention with causal mask\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=inputs,\n",
        "            mask=mask,\n",
        "            decode=False\n",
        "        )\n",
        "        attention_output = self.dropout1(attention_output, deterministic=not training)\n",
        "        out1 = self.layer_norm1(inputs + attention_output)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = self.linear1(out1)\n",
        "        ffn_output = nnx.relu(ffn_output)\n",
        "        ffn_output = self.linear2(ffn_output)\n",
        "        ffn_output = self.dropout2(ffn_output, deterministic=not training)\n",
        "\n",
        "        return self.layer_norm2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return token_embedding + position_embedding\n",
        "\n",
        "\n",
        "class MiniGPT(nnx.Module):\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    maxlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        self.transformer_blocks = [TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
        "        ) for _ in range(num_transformer_blocks)]\n",
        "\n",
        "        self.output_layer = nnx.Linear(in_features=embed_dim,\n",
        "                                       out_features=vocab_size,\n",
        "                                       kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                       bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                       rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)\n",
        "        outputs = self.output_layer(x)\n",
        "        return outputs\n",
        "\n",
        "    def generate_text(self, max_tokens: int, start_tokens: [int], top_k=10):\n",
        "        def sample_from(logits):\n",
        "            logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "            logits = nnx.softmax(logits)\n",
        "            return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "        def generate_step(start_tokens):\n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            if pad_len < 0:\n",
        "                x = jnp.array(start_tokens[:maxlen])\n",
        "                sample_index = maxlen - 1\n",
        "            elif pad_len > 0:\n",
        "                x = jnp.array(start_tokens + [0] * pad_len)\n",
        "            else:\n",
        "                x = jnp.array(start_tokens)\n",
        "\n",
        "            x = x[None, :]\n",
        "            logits = self(x)\n",
        "            next_token = sample_from(logits[0][sample_index])\n",
        "            return next_token\n",
        "\n",
        "        generated = []\n",
        "        for _ in range(max_tokens):\n",
        "            next_token = generate_step(start_tokens + generated)\n",
        "            # Truncate whatever is after '<|endoftext|>' (stop word)\n",
        "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
        "              break\n",
        "            generated.append(int(next_token))\n",
        "        return tokenizer.decode(start_tokens + generated)\n",
        "\n",
        "def create_model(rngs):\n",
        "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks=4, rngs=rngs)"
      ],
      "metadata": {
        "id": "i3ydZHLNQDBl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "num_transformer_blocks = 8\n",
        "maxlen = 256\n",
        "embed_dim = 256\n",
        "num_heads = 8\n",
        "feed_forward_dim = 256\n",
        "batch_size = 256 # You can set a bigger batch size if using Kaggle TPU\n",
        "num_epochs = 1"
      ],
      "metadata": {
        "id": "14yifrw0QOmN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TextDataset:\n",
        "    data: list\n",
        "    maxlen: int\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # Use Tiktoken for tokenization\n",
        "        encoding = tokenizer.encode(self.data[idx], allowed_special={'<|endoftext|>'})[:self.maxlen]  # Tokenize and truncate\n",
        "        return encoding + [0] * (self.maxlen - len(encoding))  # Pad to maxlen\n",
        "\n",
        "def load_and_preprocess_data(file_path, batch_size, maxlen):\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "      text = f.read()\n",
        "\n",
        "    stories = text.split('<|endoftext|>')\n",
        "    stories = [story+'<|endoftext|>' for story in stories if story.strip()]\n",
        "    df = pd.DataFrame({'text': stories})\n",
        "    data = df['text'].dropna().tolist()\n",
        "    dataset = TextDataset(data, maxlen)\n",
        "\n",
        "    sampler = pygrain.IndexSampler(\n",
        "        len(dataset),\n",
        "        shuffle=False,\n",
        "        seed=42,\n",
        "        shard_options=pygrain.NoSharding(),\n",
        "        num_epochs=num_epochs,\n",
        "    )\n",
        "\n",
        "    dl = pygrain.DataLoader(\n",
        "        data_source=dataset,\n",
        "        sampler=sampler,\n",
        "        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
        "    )\n",
        "\n",
        "    return dl\n",
        "\n",
        "text_dl = load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen)"
      ],
      "metadata": {
        "id": "q5TLqSQrQLMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ],
      "metadata": {
        "id": "-SQlT3MHQE42"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}