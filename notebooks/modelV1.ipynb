{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QrWmX-EyN3um"
      },
      "outputs": [],
      "source": [
        "# !pip install -q jax-ai-stack\n",
        "# !pip install -Uq tiktoken grain matplotlib\n",
        "# !pip install grain\n",
        "\n",
        "\n",
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5M2Cxi_6N3uo"
      },
      "outputs": [],
      "source": [
        "# import jax\n",
        "# jax.devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-jGF3qn0N3up"
      },
      "outputs": [],
      "source": [
        "# !wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true -O TinyStories-train.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sBPCSlCeN3uq"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax\n",
        "from dataclasses import dataclass\n",
        "import grain.python as pygrain\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kwLYPHmJN3uq"
      },
      "outputs": [],
      "source": [
        "mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TvE_CdExN3ur"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
        "                                          in_features=embed_dim,\n",
        "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                          rngs=rngs)\n",
        "        self.dropout1 = nnx.Dropout(rate=rate)\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
        "                                  out_features=ff_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
        "                                  out_features=embed_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  rngs=rngs)\n",
        "        self.dropout2 = nnx.Dropout(rate=rate)\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         rngs=rngs)\n",
        "\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        _, seq_len, _ = input_shape\n",
        "\n",
        "        # Create causal mask\n",
        "        mask = causal_attention_mask(seq_len)\n",
        "\n",
        "        # Apply MultiHeadAttention with causal mask\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=inputs,\n",
        "            mask=mask,\n",
        "            decode=False\n",
        "        )\n",
        "        attention_output = self.dropout1(attention_output, deterministic=not training)\n",
        "        out1 = self.layer_norm1(inputs + attention_output)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = self.linear1(out1)\n",
        "        ffn_output = nnx.relu(ffn_output)\n",
        "        ffn_output = self.linear2(ffn_output)\n",
        "        ffn_output = self.dropout2(ffn_output, deterministic=not training)\n",
        "\n",
        "        return self.layer_norm2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return token_embedding + position_embedding\n",
        "\n",
        "\n",
        "class MiniGPT(nnx.Module):\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    maxlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        self.transformer_blocks = [TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
        "        ) for _ in range(num_transformer_blocks)]\n",
        "\n",
        "        self.output_layer = nnx.Linear(in_features=embed_dim,\n",
        "                                       out_features=vocab_size,\n",
        "                                       kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                       bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                       rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)\n",
        "        outputs = self.output_layer(x)\n",
        "        return outputs\n",
        "\n",
        "    def generate_text(self, max_tokens: int, start_tokens: [int], top_k=10):\n",
        "        def sample_from(logits):\n",
        "            logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "            logits = nnx.softmax(logits)\n",
        "            return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "        def generate_step(start_tokens):\n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            if pad_len < 0:\n",
        "                x = jnp.array(start_tokens[:maxlen])\n",
        "                sample_index = maxlen - 1\n",
        "            elif pad_len > 0:\n",
        "                x = jnp.array(start_tokens + [0] * pad_len)\n",
        "            else:\n",
        "                x = jnp.array(start_tokens)\n",
        "\n",
        "            x = x[None, :]\n",
        "            logits = self(x)\n",
        "            next_token = sample_from(logits[0][sample_index])\n",
        "            return next_token\n",
        "\n",
        "        generated = []\n",
        "        for _ in range(max_tokens):\n",
        "            next_token = generate_step(start_tokens + generated)\n",
        "            # Truncate whatever is after '<|endoftext|>' (stop word)\n",
        "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
        "              break\n",
        "            generated.append(int(next_token))\n",
        "        return tokenizer.decode(start_tokens + generated)\n",
        "\n",
        "def create_model(rngs):\n",
        "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks=4, rngs=rngs)"
      ],
      "metadata": {
        "id": "i3ydZHLNQDBl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "num_transformer_blocks = 8\n",
        "maxlen = 256\n",
        "embed_dim = 256\n",
        "num_heads = 8\n",
        "feed_forward_dim = 256\n",
        "batch_size = 256 # You can set a bigger batch size if using Kaggle TPU\n",
        "num_epochs = 1"
      ],
      "metadata": {
        "id": "14yifrw0QOmN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TextDataset:\n",
        "    data: list\n",
        "    maxlen: int\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # Use Tiktoken for tokenization\n",
        "        encoding = tokenizer.encode(self.data[idx], allowed_special={'<|endoftext|>'})[:self.maxlen]  # Tokenize and truncate\n",
        "        return encoding + [0] * (self.maxlen - len(encoding))  # Pad to maxlen\n",
        "\n",
        "def load_and_preprocess_data(file_path, batch_size, maxlen):\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "      text = f.read()\n",
        "\n",
        "    stories = text.split('<|endoftext|>')\n",
        "    stories = [story+'<|endoftext|>' for story in stories if story.strip()]\n",
        "    df = pd.DataFrame({'text': stories})\n",
        "    data = df['text'].dropna().tolist()\n",
        "    dataset = TextDataset(data, maxlen)\n",
        "\n",
        "    sampler = pygrain.IndexSampler(\n",
        "        len(dataset),\n",
        "        shuffle=False,\n",
        "        seed=42,\n",
        "        shard_options=pygrain.NoSharding(),\n",
        "        num_epochs=num_epochs,\n",
        "    )\n",
        "\n",
        "    dl = pygrain.DataLoader(\n",
        "        data_source=dataset,\n",
        "        sampler=sampler,\n",
        "        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
        "    )\n",
        "\n",
        "    return dl\n",
        "\n",
        "text_dl = load_and_preprocess_data('smalldata.txt', batch_size, maxlen)"
      ],
      "metadata": {
        "id": "q5TLqSQrQLMU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ],
      "metadata": {
        "id": "-SQlT3MHQE42"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(rngs=nnx.Rngs(0))\n",
        "optimizer = nnx.Optimizer(model, optax.adam(1e-3))\n",
        "metrics = nnx.MultiMetric(\n",
        "  loss=nnx.metrics.Average('loss'),\n",
        ")\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start_prompt = \"Once upon a time\"\n",
        "start_tokens = tokenizer.encode(start_prompt)[:maxlen]\n",
        "generated_text = model.generate_text(\n",
        "    maxlen, start_tokens\n",
        ")\n",
        "print(f\"Initial generated text:\\n{generated_text}\\n\")\n",
        "\n",
        "\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "}\n",
        "\n",
        "prep_target_batch = jax.vmap(lambda tokens: jnp.concatenate((tokens[1:], jnp.array([0]))))\n",
        "\n",
        "step = 0\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    for batch in text_dl:\n",
        "        if len(batch) % len(jax.devices()) != 0:\n",
        "          continue  # skip the remaining elements\n",
        "        input_batch = jnp.array(jnp.array(batch).T)\n",
        "        target_batch = prep_target_batch(input_batch)\n",
        "        train_step(model, optimizer, metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
        "\n",
        "        if (step + 1) % 200 == 0:\n",
        "          for metric, value in metrics.compute().items():\n",
        "              metrics_history[f'train_{metric}'].append(value)\n",
        "          metrics.reset()\n",
        "\n",
        "          elapsed_time = time.time() - start_time\n",
        "          print(f\"Step {step + 1}, Loss: {metrics_history['train_loss'][-1]}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
        "          start_time = time.time()\n",
        "\n",
        "          generated_text = model.generate_text(\n",
        "              maxlen, start_tokens\n",
        "          )\n",
        "          print(f\"Generated text:\\n{generated_text}\\n\")\n",
        "        step += 1\n",
        "\n",
        "# Final text generation\n",
        "generated_text = model.generate_text(\n",
        "    maxlen, start_tokens\n",
        ")\n",
        "print(f\"Final generated text:\\n{generated_text}\")"
      ],
      "metadata": {
        "id": "shxoBuuqQmt9",
        "outputId": "ab99d68b-66db-4414-92a6-28689782774a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial generated text:\n",
            "Once upon a timeaciaGender gearuser Analysisval {} Bruce Lauren helic Lauren Bruce againstliterally SQU retire Path {}valascript northwest {} Bruceuit Pathascript northwestdrops freelyvic996 curated hysteria FS psychedelic {}ligaval {} perished {} Woman {} {} inflicwaves Analysis996 servants Bruce Charlotte dissatisfiedascript Whites retire {} retire {} perished Schools {}Nick sorrow psychedelicfingerelaide {} perished Blackburnrestyy elder swing interesting Tang Shattered {} {}scl less abnorm abnorm Goldmancre motions fleshthird fatal haw {} perished {} Dani Whites {} Whites retire {} sorrowolate Whites {} Womanlast doom appendixfinger {}Rain pressurefinger {} {} Womanorem Bruce simplyrest {} {} inexplicablerest HS Charlotte dissatisfied stranger perished Tang reassUV {} Abortion outragedaligned {} server fertility decision FSShould {} {} northwestroid variable {} Whites {} dancersithmetic {} {} Whites las hampered {} Woman {} Woman {} Mineundrum {} inexplicable taking Keithrest {} {} retire interesting Tangresterv whole Kyundrum survivor {}val decision taking aggression {} acidicundrum salesmanundrumOSE lasManager twins Pathrest {}urnedrestEN {} Runtime {} perished Brigham Analysis developerscre Atom {}scl HouthInteger {} northwest appease 403 {} perished THR Hyundai Captainilipp {} inflic decision {} inflic {} retire {} Whites dancers {}scl FS lore appease Din {} Whites abnorm[] {} {}val {} specialist enchantExcept pressure psychedelicstepsundrumOhioOhioOhioOhio\n",
            "\n",
            "Step 200, Loss: 4.827099323272705, Elapsed Time: 112.79 seconds\n",
            "Generated text:\n",
            "Once upon a time a a little. She little. She was. She little. She was.\n",
            ". She little.\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ". She little.\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".!!!!\n",
            "\n",
            "Step 400, Loss: 3.7938005924224854, Elapsed Time: 102.36 seconds\n",
            "Generated text:\n",
            "Once upon a time there was a girl named She loved. She was years day. She was day. She was day. She was day. She was day. She was day. One day. She was day. She was day. She was day. She was day.\n",
            " wanted.\n",
            " play. She was day.\n",
            " was so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so!!!!\n",
            "\n",
            "Step 600, Loss: 3.35459303855896, Elapsed Time: 84.05 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YsbAopnXQp0z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}