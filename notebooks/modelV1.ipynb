{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isatyamks/jax-flax-llm-examples/blob/main/notebooks/modelV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "QrWmX-EyN3um"
      },
      "outputs": [],
      "source": [
        "# !pip install -q jax-ai-stack\n",
        "# !pip install -Uq tiktoken grain matplotlib\n",
        "# !pip install grain\n",
        "\n",
        "\n",
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5M2Cxi_6N3uo"
      },
      "outputs": [],
      "source": [
        "# import jax\n",
        "# jax.devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "-jGF3qn0N3up"
      },
      "outputs": [],
      "source": [
        "# !wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true -O TinyStories-train.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "sBPCSlCeN3uq"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax\n",
        "from dataclasses import dataclass\n",
        "import grain.python as pygrain\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "kwLYPHmJN3uq"
      },
      "outputs": [],
      "source": [
        "mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "TvE_CdExN3ur"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
        "                                          in_features=embed_dim,\n",
        "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                          rngs=rngs)\n",
        "        self.dropout1 = nnx.Dropout(rate=rate)\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
        "                                  out_features=ff_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
        "                                  out_features=embed_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  rngs=rngs)\n",
        "        self.dropout2 = nnx.Dropout(rate=rate)\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         rngs=rngs)\n",
        "\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        _, seq_len, _ = input_shape\n",
        "\n",
        "        # Create causal mask\n",
        "        mask = causal_attention_mask(seq_len)\n",
        "\n",
        "        # Apply MultiHeadAttention with causal mask\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=inputs,\n",
        "            mask=mask,\n",
        "            decode=False\n",
        "        )\n",
        "        attention_output = self.dropout1(attention_output, deterministic=not training)\n",
        "        out1 = self.layer_norm1(inputs + attention_output)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = self.linear1(out1)\n",
        "        ffn_output = nnx.relu(ffn_output)\n",
        "        ffn_output = self.linear2(ffn_output)\n",
        "        ffn_output = self.dropout2(ffn_output, deterministic=not training)\n",
        "\n",
        "        return self.layer_norm2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return token_embedding + position_embedding\n",
        "\n",
        "\n",
        "class MiniGPT(nnx.Module):\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    maxlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        self.transformer_blocks = [TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
        "        ) for _ in range(num_transformer_blocks)]\n",
        "\n",
        "        self.output_layer = nnx.Linear(in_features=embed_dim,\n",
        "                                       out_features=vocab_size,\n",
        "                                       kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                       bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                       rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)\n",
        "        outputs = self.output_layer(x)\n",
        "        return outputs\n",
        "\n",
        "    def generate_text(self, max_tokens: int, start_tokens: [int], top_k=10):\n",
        "        def sample_from(logits):\n",
        "            logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "            logits = nnx.softmax(logits)\n",
        "            return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "        def generate_step(start_tokens):\n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            if pad_len < 0:\n",
        "                x = jnp.array(start_tokens[:maxlen])\n",
        "                sample_index = maxlen - 1\n",
        "            elif pad_len > 0:\n",
        "                x = jnp.array(start_tokens + [0] * pad_len)\n",
        "            else:\n",
        "                x = jnp.array(start_tokens)\n",
        "\n",
        "            x = x[None, :]\n",
        "            logits = self(x)\n",
        "            next_token = sample_from(logits[0][sample_index])\n",
        "            return next_token\n",
        "\n",
        "        generated = []\n",
        "        for _ in range(max_tokens):\n",
        "            next_token = generate_step(start_tokens + generated)\n",
        "            # Truncate whatever is after '<|endoftext|>' (stop word)\n",
        "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
        "              break\n",
        "            generated.append(int(next_token))\n",
        "        return tokenizer.decode(start_tokens + generated)\n",
        "\n",
        "def create_model(rngs):\n",
        "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks=4, rngs=rngs)"
      ],
      "metadata": {
        "id": "i3ydZHLNQDBl"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "num_transformer_blocks = 8\n",
        "maxlen = 256\n",
        "embed_dim = 256\n",
        "num_heads = 8\n",
        "feed_forward_dim = 256\n",
        "batch_size = 256 # You can set a bigger batch size if using Kaggle TPU\n",
        "num_epochs = 1"
      ],
      "metadata": {
        "id": "14yifrw0QOmN"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TextDataset:\n",
        "    data: list\n",
        "    maxlen: int\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # Use Tiktoken for tokenization\n",
        "        encoding = tokenizer.encode(self.data[idx], allowed_special={'<|endoftext|>'})[:self.maxlen]  # Tokenize and truncate\n",
        "        return encoding + [0] * (self.maxlen - len(encoding))  # Pad to maxlen\n",
        "\n",
        "def load_and_preprocess_data(file_path, batch_size, maxlen):\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "      text = f.read()\n",
        "\n",
        "    stories = text.split('<|endoftext|>')\n",
        "    stories = [story+'<|endoftext|>' for story in stories if story.strip()]\n",
        "    df = pd.DataFrame({'text': stories})\n",
        "    data = df['text'].dropna().tolist()\n",
        "    dataset = TextDataset(data, maxlen)\n",
        "\n",
        "    sampler = pygrain.IndexSampler(\n",
        "        len(dataset),\n",
        "        shuffle=False,\n",
        "        seed=42,\n",
        "        shard_options=pygrain.NoSharding(),\n",
        "        num_epochs=num_epochs,\n",
        "    )\n",
        "\n",
        "    dl = pygrain.DataLoader(\n",
        "        data_source=dataset,\n",
        "        sampler=sampler,\n",
        "        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
        "    )\n",
        "\n",
        "    return dl\n",
        "\n",
        "text_dl = load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen)"
      ],
      "metadata": {
        "id": "q5TLqSQrQLMU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ],
      "metadata": {
        "id": "-SQlT3MHQE42"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(rngs=nnx.Rngs(0))\n",
        "optimizer = nnx.Optimizer(model, optax.adam(1e-3))\n",
        "metrics = nnx.MultiMetric(\n",
        "  loss=nnx.metrics.Average('loss'),\n",
        ")\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start_prompt = \"Once upon a time\"\n",
        "start_tokens = tokenizer.encode(start_prompt)[:maxlen]\n",
        "generated_text = model.generate_text(\n",
        "    maxlen, start_tokens\n",
        ")\n",
        "print(f\"Initial generated text:\\n{generated_text}\\n\")\n",
        "\n",
        "\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "}\n",
        "\n",
        "prep_target_batch = jax.vmap(lambda tokens: jnp.concatenate((tokens[1:], jnp.array([0]))))\n",
        "\n",
        "step = 0\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    for batch in text_dl:\n",
        "        if len(batch) % len(jax.devices()) != 0:\n",
        "          continue  # skip the remaining elements\n",
        "        input_batch = jnp.array(jnp.array(batch).T)\n",
        "        target_batch = prep_target_batch(input_batch)\n",
        "        train_step(model, optimizer, metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
        "\n",
        "        if (step + 1) % 200 == 0:\n",
        "          for metric, value in metrics.compute().items():\n",
        "              metrics_history[f'train_{metric}'].append(value)\n",
        "          metrics.reset()\n",
        "\n",
        "          elapsed_time = time.time() - start_time\n",
        "          print(f\"Step {step + 1}, Loss: {metrics_history['train_loss'][-1]}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
        "          start_time = time.time()\n",
        "\n",
        "          generated_text = model.generate_text(\n",
        "              maxlen, start_tokens\n",
        "          )\n",
        "          print(f\"Generated text:\\n{generated_text}\\n\")\n",
        "        step += 1\n",
        "\n",
        "# Final text generation\n",
        "generated_text = model.generate_text(\n",
        "    maxlen, start_tokens\n",
        ")\n",
        "print(f\"Final generated text:\\n{generated_text}\")"
      ],
      "metadata": {
        "id": "shxoBuuqQmt9",
        "outputId": "dd1e0d9a-b4eb-46cf-b325-34229990411b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial generated text:\n",
            "Once upon a timeaciaGender gearuser Analysisval {} Bruce Lauren helic Lauren Bruce againstliterally SQU retire Path {}valascript northwest {} Bruceuit Pathascript northwestdrops freelyvic996 curated hysteria FS psychedelic {}ligaval {} perished {} Woman {} {} inflicwaves Analysis996 servants Bruce Charlotte dissatisfiedascript Whites retire {} retire {} perished Schools {}Nick sorrow psychedelicfingerelaide {} perished Blackburnrestyy elder swing interesting Tang Shattered {} {}scl less abnorm abnorm Goldmancre motions fleshthird fatal haw {} perished {} Dani Whites {} Whites retire {} sorrowolate Whites {} Womanlast doom appendixfinger {}Rain pressurefinger {} {} Womanorem Bruce simplyrest {} {} inexplicablerest HS Charlotte dissatisfied stranger perished Tang reassUV {} Abortion outragedaligned {} server fertility decision FSShould {} {} northwestroid variable {} Whites {} dancersithmetic {} {} Whites las hampered {} Woman {} Woman {} Mineundrum {} inexplicable taking Keithrest {} {} retire interesting Tangresterv whole Kyundrum survivor {}val decision taking aggression {} acidicundrum salesmanundrumOSE lasManager twins Pathrest {}urnedrestEN {} Runtime {} perished Brigham Analysis developerscre Atom {}scl HouthInteger {} northwest appease 403 {} perished THR Hyundai Captainilipp {} inflic decision {} inflic {} retire {} Whites dancers {}scl FS lore appease Din {} Whites abnorm[] {} {}val {} specialist enchantExcept pressure psychedelicstepsundrumOhioOhioOhioOhio\n",
            "\n",
            "Step 200, Loss: 4.827099323272705, Elapsed Time: 112.83 seconds\n",
            "Generated text:\n",
            "Once upon a time a a little. She little. She was. She little. She was.\n",
            ". She little.\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ". She little.\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".!!!!\n",
            "\n",
            "Step 400, Loss: 3.7938005924224854, Elapsed Time: 96.57 seconds\n",
            "Generated text:\n",
            "Once upon a time there was a girl named She loved. She was years day. She was day. She was day. She was day. She was day. She was day. One day. She was day. She was day. She was day. She was day.\n",
            " wanted.\n",
            " play. She was day.\n",
            " was so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so!!!!\n",
            "\n",
            "Step 600, Loss: 3.35459303855896, Elapsed Time: 84.93 seconds\n",
            "Generated text:\n",
            "Once upon a time there was a little girl named Lily was very day, Lily. She loved to play with the park. One day, she saw a big big big big big, \"Let's mom said, \"Let's mom said, \"Let's mom said, \"Let's mom said, \"Let's mom said.\n",
            "\"Let's mom said, \"I'm a big.\n",
            "\"Let's mom said.\n",
            "\"I'm a, \"I'm a big.\n",
            "\"I'm a big, \"I can be, \"I'm a big.\n",
            "\"I'm a big.\n",
            "\"I'm a big, \"I'm happy.\n",
            "\"I'm a big.\n",
            "\"I'm happy.\n",
            "\"I'm a little girl said.\n",
            "\"I'm happy the other, \"I'm a little girl said.\n",
            "\"I can be, \"I'm happy.\n",
            "\"I'm happy.\n",
            "\"I'm so happy.\n",
            "\"I can be happy.\n",
            "\"I'm happy.\n",
            "\"I'm happy.\n",
            "\"I'm happy the other, \"I'm happy.\n",
            "The other, \"I'm happy.\n",
            "\"I'm so happy.\n",
            "The other, \"I can be happy the!!!!\n",
            "\n",
            "Step 800, Loss: 3.00892972946167, Elapsed Time: 85.77 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Timmy. Timmy loved to play with his mommy's mommy's mommy's mommy's mommy was very sad. Timmy was very sad. Timmy wanted to play with her mommy didn't want to play with her mommy didn't want to play with her mommy's mommy didn't want to play with her mommy's mommy's mommy's mommy didn't want to be careful.\n",
            "Lily's okay.\n",
            "Timmy's mommy's mommy didn't want to be careful.\n",
            "Timmy didn't want to be careful.\n",
            "Timmy was sad.\n",
            "Timmy was sad.\n",
            "\n",
            "\n",
            "Step 1000, Loss: 2.676360607147217, Elapsed Time: 60.30 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her toys. One day, Lily's mom said, \"Mom, Lily, Lily, Lily, Lily, Lily, Lily's mom said, \"I can't have to play with her mom. Lily's mom said, Lily's mom said, \"I can't have to the toys.\" Lily's mom said, \"I can't worry, Lily. Lily's a toy car. Lily's a toy car. Lily's a big, Lily's a big, \"I like to play with Lily. Lily's play with her toys and Lily's mom. Lily's mom said, \"I love you like you like to play with her toys.\" Lily's mom and Lily's mom said, \"Thank you, Lily. Lily. Lily's a toy car. Lily's a toy car. Lily's a toy car. Lily's a toy car. Lily's mom and Lily's mom and Lily's mom said, \"Thank you can have to play with her mom. Lily. Lily's play with her mom and Lily's mom and Lily's mom and Lily's mom and Lily and Lily. Lily and Lily's mom and Lily's mom and Lily's mom and!!!!\n",
            "\n",
            "Step 1200, Loss: 2.4030351638793945, Elapsed Time: 85.06 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and play with her friends. One day, Lily's mom said, \"Lily, Lily, I want to play with you, Lily.\n",
            "Lily's mom said, \"I don't worry, Lily's a big, Lily's a big dog.\"\n",
            "Lily was sad and wanted to play with her friends. Lily was sad. She wanted to help her mom and Lily felt better.\n",
            "Lily's mom said, \"Don't worry, Lily. I will help you.\" Lily's mom said, \"Okay, Lily. I will help you.\"\n",
            "Lily and Lily's mom said, \"Okay, Lily. They are very good for you, Lily. They are very good for helping Lily. They played together and played together every day.\n",
            "\n",
            "\n",
            "Step 1400, Loss: 2.1864371299743652, Elapsed Time: 66.84 seconds\n",
            "Generated text:\n",
            "Once upon a time there was a little girl named Lily. She loved to play with her toys, but she was always very happy. One day, she went to the park with her mom.\n",
            "Lily saw a big, shiny ball on the ground. She wanted to play with it, but it was too hard. She tried to pull it up the ball.\n",
            "Lily tried to pull it up the ball. She tried to pull it up the ball. She tried to pull it up the ball.\n",
            "Suddenly, the ball broke. It was very sad.\n",
            "Lily was sad. She wanted to help her mom. She said, \"Don't worry, I will help you.\"\n",
            "Lily was sad. She said, \"Don't worry, I will help you.\"\n",
            "Lily felt sad. She said, \"Don't worry, let's go to the park. It will be fun.\"\n",
            "\n",
            "\n",
            "Step 1600, Loss: 2.0228700637817383, Elapsed Time: 71.07 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her friends. One day, she saw a big, scary monster in the park. The monster was very sad and didn't know what to do.\n",
            "Lily's mom told her that she wanted to help her mom. So, she went to the park and found a big, scary dog. The monster was very sad and didn't want to play with the monster. But the monster didn't want to play with the monster. So, the monster ran away and started to cry.\n",
            "Lily and her mom went to the monster and asked, \"Why did you do not answer?\" Lily asked. Lily felt sad and sad. The monster said, \"Don't worry, Lily. I'm sorry, but I didn't mean to hurt you. I didn't mean to hurt you.\"\n",
            "Lily learned that it's important to listen to her mom and not listen to her mom. From that day on, Lily learned that sometimes things not to be kind and not to others.\n",
            "\n",
            "\n",
            "Step 1800, Loss: 1.9058681726455688, Elapsed Time: 77.82 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(metrics_history['train_loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YsbAopnXQp0z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "ef3a2b75-ca25-4f28-8d1b-e68ded7e0afe"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import eqx\n",
        "import cloudpickle\n",
        "import os\n",
        "\n",
        "# Define file paths for saving the model and metadata.\n",
        "checkpoint_file = os.path.join(\"checkpoints\", \"model.eqx\")\n",
        "checkpoint_params_file = os.path.join(\"checkpoints\", \"params.pkl\")\n",
        "\n",
        "# Ensure the checkpoint directory exists.\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "# Save the model using Equinox’s tree serialization.\n",
        "eqx.tree_serialise_leaves(checkpoint_file, model)\n",
        "\n",
        "# Save additional parameters (like training step, learning rate, etc.) in a dictionary.\n",
        "checkpoint_params = {\n",
        "    \"iter_num\": step,\n",
        "    \"train_loss\": metrics_history['train_loss'][-1] if metrics_history['train_loss'] else None,\n",
        "    # You can add other parameters like optimizer state or config if needed.\n",
        "}\n",
        "\n",
        "with open(checkpoint_params_file, \"wb\") as f:\n",
        "    cloudpickle.dump(checkpoint_params, f)\n"
      ],
      "metadata": {
        "id": "Z4kjN7VgUeg4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}